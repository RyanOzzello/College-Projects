{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activities From Applied Deep Learning with PyTorch\n",
    "Book's Github link: https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 1 on pg 17: Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dummy data\n",
    "num_samples = 100\n",
    "num_features = 5\n",
    "data = torch.randn(num_samples, num_features)\n",
    "labels = torch.randint(0,2,(num_samples,1)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = nn.Sequential(nn.Linear(num_features, 1),\n",
    "                      nn.Sigmoid())\n",
    "MSE = torch.nn.MSELoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "iterations = 100\n",
    "losses = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    preds = model(data)\n",
    "    loss = MSE(preds, labels)\n",
    "    losses.append(loss.item())\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 2 on pg 50 in Chapter 2: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2001</td>\n",
       "      <td>49.94357</td>\n",
       "      <td>21.47114</td>\n",
       "      <td>73.07750</td>\n",
       "      <td>8.74861</td>\n",
       "      <td>-17.40628</td>\n",
       "      <td>-13.09905</td>\n",
       "      <td>-25.01202</td>\n",
       "      <td>-12.23257</td>\n",
       "      <td>7.83089</td>\n",
       "      <td>...</td>\n",
       "      <td>13.01620</td>\n",
       "      <td>-54.40548</td>\n",
       "      <td>58.99367</td>\n",
       "      <td>15.37344</td>\n",
       "      <td>1.11144</td>\n",
       "      <td>-23.08793</td>\n",
       "      <td>68.40795</td>\n",
       "      <td>-1.82223</td>\n",
       "      <td>-27.46348</td>\n",
       "      <td>2.26327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>48.73215</td>\n",
       "      <td>18.42930</td>\n",
       "      <td>70.32679</td>\n",
       "      <td>12.94636</td>\n",
       "      <td>-10.32437</td>\n",
       "      <td>-24.83777</td>\n",
       "      <td>8.76630</td>\n",
       "      <td>-0.92019</td>\n",
       "      <td>18.76548</td>\n",
       "      <td>...</td>\n",
       "      <td>5.66812</td>\n",
       "      <td>-19.68073</td>\n",
       "      <td>33.04964</td>\n",
       "      <td>42.87836</td>\n",
       "      <td>-9.90378</td>\n",
       "      <td>-32.22788</td>\n",
       "      <td>70.49388</td>\n",
       "      <td>12.04941</td>\n",
       "      <td>58.43453</td>\n",
       "      <td>26.92061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2001</td>\n",
       "      <td>50.95714</td>\n",
       "      <td>31.85602</td>\n",
       "      <td>55.81851</td>\n",
       "      <td>13.41693</td>\n",
       "      <td>-6.57898</td>\n",
       "      <td>-18.54940</td>\n",
       "      <td>-3.27872</td>\n",
       "      <td>-2.35035</td>\n",
       "      <td>16.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>3.03800</td>\n",
       "      <td>26.05866</td>\n",
       "      <td>-50.92779</td>\n",
       "      <td>10.93792</td>\n",
       "      <td>-0.07568</td>\n",
       "      <td>43.20130</td>\n",
       "      <td>-115.00698</td>\n",
       "      <td>-0.05859</td>\n",
       "      <td>39.67068</td>\n",
       "      <td>-0.66345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2001</td>\n",
       "      <td>48.24750</td>\n",
       "      <td>-1.89837</td>\n",
       "      <td>36.29772</td>\n",
       "      <td>2.58776</td>\n",
       "      <td>0.97170</td>\n",
       "      <td>-26.21683</td>\n",
       "      <td>5.05097</td>\n",
       "      <td>-10.34124</td>\n",
       "      <td>3.55005</td>\n",
       "      <td>...</td>\n",
       "      <td>34.57337</td>\n",
       "      <td>-171.70734</td>\n",
       "      <td>-16.96705</td>\n",
       "      <td>-46.67617</td>\n",
       "      <td>-12.51516</td>\n",
       "      <td>82.58061</td>\n",
       "      <td>-72.08993</td>\n",
       "      <td>9.90558</td>\n",
       "      <td>199.62971</td>\n",
       "      <td>18.85382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2001</td>\n",
       "      <td>50.97020</td>\n",
       "      <td>42.20998</td>\n",
       "      <td>67.09964</td>\n",
       "      <td>8.46791</td>\n",
       "      <td>-15.85279</td>\n",
       "      <td>-16.81409</td>\n",
       "      <td>-12.48207</td>\n",
       "      <td>-9.37636</td>\n",
       "      <td>12.63699</td>\n",
       "      <td>...</td>\n",
       "      <td>9.92661</td>\n",
       "      <td>-55.95724</td>\n",
       "      <td>64.92712</td>\n",
       "      <td>-17.72522</td>\n",
       "      <td>-1.49237</td>\n",
       "      <td>-7.50035</td>\n",
       "      <td>51.76631</td>\n",
       "      <td>7.88713</td>\n",
       "      <td>55.66926</td>\n",
       "      <td>28.74903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2         3         4         5         6         7   \\\n",
       "0  2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905 -25.01202   \n",
       "1  2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   8.76630   \n",
       "2  2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940  -3.27872   \n",
       "3  2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   5.05097   \n",
       "4  2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409 -12.48207   \n",
       "\n",
       "         8         9   ...        81         82        83        84        85  \\\n",
       "0 -12.23257   7.83089  ...  13.01620  -54.40548  58.99367  15.37344   1.11144   \n",
       "1  -0.92019  18.76548  ...   5.66812  -19.68073  33.04964  42.87836  -9.90378   \n",
       "2  -2.35035  16.07017  ...   3.03800   26.05866 -50.92779  10.93792  -0.07568   \n",
       "3 -10.34124   3.55005  ...  34.57337 -171.70734 -16.96705 -46.67617 -12.51516   \n",
       "4  -9.37636  12.63699  ...   9.92661  -55.95724  64.92712 -17.72522  -1.49237   \n",
       "\n",
       "         86         87        88         89        90  \n",
       "0 -23.08793   68.40795  -1.82223  -27.46348   2.26327  \n",
       "1 -32.22788   70.49388  12.04941   58.43453  26.92061  \n",
       "2  43.20130 -115.00698  -0.05859   39.67068  -0.66345  \n",
       "3  82.58061  -72.08993   9.90558  199.62971  18.85382  \n",
       "4  -7.50035   51.76631   7.88713   55.66926  28.74903  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data\n",
    "data = pd.read_csv('YearPredictionMSD.txt', header=None, nrows=50000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2001.00000\n",
      "1       49.94357\n",
      "2       21.47114\n",
      "3       73.07750\n",
      "4        8.74861\n",
      "         ...    \n",
      "86     -23.08793\n",
      "87      68.40795\n",
      "88      -1.82223\n",
      "89     -27.46348\n",
      "90       2.26327\n",
      "Name: 0, Length: 91, dtype: float64\n",
      "\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check if some columns have non-numeric data\n",
    "cols = data.iloc[0,:]\n",
    "print(cols)\n",
    "print('\\ndtype: ' + str(cols.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Numeric Data\n"
     ]
    }
   ],
   "source": [
    "# check if numeric\n",
    "numeric_column = data.apply(lambda s: pd.to_numeric(s, errors='coerce').notnull().all())\n",
    "nonnumeric_columns = numeric_column.index[numeric_column == False].tolist()\n",
    "if not nonnumeric_columns:\n",
    "    print('All Numeric Data')\n",
    "else:\n",
    "    print(f'Columns with non-numeric values are: {nonnumeric_columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values\n"
     ]
    }
   ],
   "source": [
    "# check if any missing values\n",
    "num_missing = data.isnull().sum().sum()\n",
    "if num_missing == 0:\n",
    "    print('No missing values')\n",
    "else:\n",
    "    print(f'There are {num_missing} missing values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '1.9%', 1: '0.96%', 2: '1.1%', 3: '1.1%', 4: '1.5%', 5: '0.8%', 6: '1.0%', 7: '0.98%', 8: '1.0%', 9: '1.1%', 10: '0.55%', 11: '1.0%', 12: '0.97%', 13: '1.5%', 14: '1.7%', 15: '1.6%', 16: '1.6%', 17: '1.4%', 18: '1.6%', 19: '1.3%', 20: '1.6%', 21: '1.5%', 22: '1.3%', 23: '1.5%', 24: '1.3%', 25: '1.8%', 26: '1.7%', 27: '1.7%', 28: '1.8%', 29: '1.7%', 30: '1.6%', 31: '1.5%', 32: '1.7%', 33: '1.6%', 34: '1.5%', 35: '1.6%', 36: '1.7%', 37: '1.7%', 38: '1.6%', 39: '1.5%', 40: '1.7%', 41: '1.7%', 42: '1.5%', 43: '1.4%', 44: '1.6%', 45: '1.5%', 46: '1.7%', 47: '1.7%', 48: '1.7%', 49: '1.5%', 50: '1.6%', 51: '1.5%', 52: '1.5%', 53: '1.5%', 54: '1.6%', 55: '1.8%', 56: '1.7%', 57: '1.6%', 58: '1.3%', 59: '1.6%', 60: '1.6%', 61: '1.5%', 62: '1.6%', 63: '1.7%', 64: '1.7%', 65: '1.6%', 66: '1.8%', 67: '1.5%', 68: '1.8%', 69: '1.6%', 70: '1.7%', 71: '1.6%', 72: '1.8%', 73: '1.5%', 74: '1.6%', 75: '1.5%', 76: '1.7%', 77: '1.8%', 78: '1.5%', 79: '1.4%', 80: '1.5%', 81: '1.8%', 82: '1.7%', 83: '1.5%', 84: '1.6%', 85: '1.5%', 86: '1.6%', 87: '1.5%', 88: '1.5%', 89: '1.7%', 90: '1.8%'}\n"
     ]
    }
   ],
   "source": [
    "outliers = {}\n",
    "for i in range(data.shape[1]):\n",
    "    min_t = data[data.columns[i]].mean() -  3 * data[data.columns[i]].std()\n",
    "    max_t = data[data.columns[i]].mean() +  3 * data[data.columns[i]].std()\n",
    "    count = 0\n",
    "    for j in data[data.columns[i]]:\n",
    "        if j < min_t or j > max_t:\n",
    "            count += 1\n",
    "    percentage = count/data.shape[0] * 100\n",
    "    outliers[data.columns[i]] = f'{percentage:.2}%'\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features and labels\n",
    "X = data.iloc[:, 1:]\n",
    "Y = data.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.082657</td>\n",
       "      <td>0.382437</td>\n",
       "      <td>1.841985</td>\n",
       "      <td>0.459652</td>\n",
       "      <td>-0.480074</td>\n",
       "      <td>-0.282606</td>\n",
       "      <td>-1.590785</td>\n",
       "      <td>-1.300854</td>\n",
       "      <td>0.378336</td>\n",
       "      <td>-0.683719</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086005</td>\n",
       "      <td>0.099339</td>\n",
       "      <td>0.148291</td>\n",
       "      <td>-0.255625</td>\n",
       "      <td>0.040944</td>\n",
       "      <td>-0.362616</td>\n",
       "      <td>0.524542</td>\n",
       "      <td>-0.467668</td>\n",
       "      <td>-0.247579</td>\n",
       "      <td>0.036872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.880874</td>\n",
       "      <td>0.321953</td>\n",
       "      <td>1.763666</td>\n",
       "      <td>0.717085</td>\n",
       "      <td>-0.165507</td>\n",
       "      <td>-1.188896</td>\n",
       "      <td>0.777905</td>\n",
       "      <td>0.122576</td>\n",
       "      <td>1.420531</td>\n",
       "      <td>0.401198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.316635</td>\n",
       "      <td>0.301448</td>\n",
       "      <td>-0.063611</td>\n",
       "      <td>0.031855</td>\n",
       "      <td>-0.655124</td>\n",
       "      <td>-0.443921</td>\n",
       "      <td>0.536517</td>\n",
       "      <td>0.573191</td>\n",
       "      <td>0.209887</td>\n",
       "      <td>1.155171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.251484</td>\n",
       "      <td>0.588929</td>\n",
       "      <td>1.350579</td>\n",
       "      <td>0.745944</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>-0.703401</td>\n",
       "      <td>-0.066747</td>\n",
       "      <td>-0.057380</td>\n",
       "      <td>1.163637</td>\n",
       "      <td>-0.090081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.399185</td>\n",
       "      <td>0.567666</td>\n",
       "      <td>-0.749508</td>\n",
       "      <td>-0.301984</td>\n",
       "      <td>-0.034072</td>\n",
       "      <td>0.227059</td>\n",
       "      <td>-0.528413</td>\n",
       "      <td>-0.335333</td>\n",
       "      <td>0.109957</td>\n",
       "      <td>-0.095865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.800148</td>\n",
       "      <td>-0.082240</td>\n",
       "      <td>0.794774</td>\n",
       "      <td>0.081829</td>\n",
       "      <td>0.336246</td>\n",
       "      <td>-1.295366</td>\n",
       "      <td>0.517369</td>\n",
       "      <td>-1.062869</td>\n",
       "      <td>-0.029679</td>\n",
       "      <td>-1.282306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590596</td>\n",
       "      <td>-0.583396</td>\n",
       "      <td>-0.472129</td>\n",
       "      <td>-0.904164</td>\n",
       "      <td>-0.820141</td>\n",
       "      <td>0.577357</td>\n",
       "      <td>-0.282033</td>\n",
       "      <td>0.412329</td>\n",
       "      <td>0.961849</td>\n",
       "      <td>0.789313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.253660</td>\n",
       "      <td>0.794806</td>\n",
       "      <td>1.671781</td>\n",
       "      <td>0.442438</td>\n",
       "      <td>-0.411071</td>\n",
       "      <td>-0.569426</td>\n",
       "      <td>-0.712128</td>\n",
       "      <td>-0.941459</td>\n",
       "      <td>0.836414</td>\n",
       "      <td>-0.160630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182976</td>\n",
       "      <td>0.090307</td>\n",
       "      <td>0.196753</td>\n",
       "      <td>-0.601570</td>\n",
       "      <td>-0.123595</td>\n",
       "      <td>-0.223957</td>\n",
       "      <td>0.429005</td>\n",
       "      <td>0.260874</td>\n",
       "      <td>0.195160</td>\n",
       "      <td>1.238096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1         2         3         4         5         6         7   \\\n",
       "0  1.082657  0.382437  1.841985  0.459652 -0.480074 -0.282606 -1.590785   \n",
       "1  0.880874  0.321953  1.763666  0.717085 -0.165507 -1.188896  0.777905   \n",
       "2  1.251484  0.588929  1.350579  0.745944  0.000857 -0.703401 -0.066747   \n",
       "3  0.800148 -0.082240  0.794774  0.081829  0.336246 -1.295366  0.517369   \n",
       "4  1.253660  0.794806  1.671781  0.442438 -0.411071 -0.569426 -0.712128   \n",
       "\n",
       "         8         9         10  ...        81        82        83        84  \\\n",
       "0 -1.300854  0.378336 -0.683719  ... -0.086005  0.099339  0.148291 -0.255625   \n",
       "1  0.122576  1.420531  0.401198  ... -0.316635  0.301448 -0.063611  0.031855   \n",
       "2 -0.057380  1.163637 -0.090081  ... -0.399185  0.567666 -0.749508 -0.301984   \n",
       "3 -1.062869 -0.029679 -1.282306  ...  0.590596 -0.583396 -0.472129 -0.904164   \n",
       "4 -0.941459  0.836414 -0.160630  ... -0.182976  0.090307  0.196753 -0.601570   \n",
       "\n",
       "         85        86        87        88        89        90  \n",
       "0  0.040944 -0.362616  0.524542 -0.467668 -0.247579  0.036872  \n",
       "1 -0.655124 -0.443921  0.536517  0.573191  0.209887  1.155171  \n",
       "2 -0.034072  0.227059 -0.528413 -0.335333  0.109957 -0.095865  \n",
       "3 -0.820141  0.577357 -0.282033  0.412329  0.961849  0.789313  \n",
       "4 -0.123595 -0.223957  0.429005  0.260874  0.195160  1.238096  \n",
       "\n",
       "[5 rows x 90 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardization\n",
    "X = (X - X.mean()) / X.std()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 90) (30000,)\n",
      "(10000, 90) (10000,)\n",
      "(10000, 90) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_shuffle = X.sample(frac=1)\n",
    "Y_shuffle = Y.sample(frac=1)\n",
    "\n",
    "x_new, x_test, y_new, y_test = train_test_split(X_shuffle, Y_shuffle, test_size=0.2, random_state=0)\n",
    "\n",
    "dev_per = x_test.shape[0]/x_new.shape[0]\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_new, y_new, test_size=dev_per, random_state=0)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_dev.shape, y_dev.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 1 on pg 55 in Chapter 2: Developing a Deep Learning Solution for a Regression Probelm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.32373719,  0.63176868, -1.45583473, ...,  0.10887368,\n",
       "        -0.24438447, -0.31329934],\n",
       "       [-0.22693673, -0.38199697,  0.38908947, ...,  0.53574038,\n",
       "        -0.05753375, -0.16755153],\n",
       "       [ 1.04128669,  0.56398328,  0.22456761, ...,  0.3073368 ,\n",
       "        -0.33685219, -0.15780187],\n",
       "       ...,\n",
       "       [ 1.26556426,  0.02321362,  0.6635287 , ..., -0.28701707,\n",
       "        -0.31211232, -0.12916517],\n",
       "       [ 0.58135667,  0.16818016,  1.5199075 , ..., -0.49193995,\n",
       "        -0.22329504,  0.24563455],\n",
       "       [-1.13809697, -1.44801413, -0.48667765, ..., -0.52175587,\n",
       "         0.24002076, -0.89662665]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tensors\n",
    "x_train = torch.Tensor(x_train.values)\n",
    "y_train = torch.Tensor(y_train.values)\n",
    "\n",
    "x_dev = torch.Tensor(x_dev.values)\n",
    "y_dev = torch.Tensor(y_dev.values)\n",
    "\n",
    "x_test = torch.Tensor(x_test.values)\n",
    "y_test = torch.Tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "input_size = x_train.shape[1]\n",
    "h0_size = 60\n",
    "h1_size = 30\n",
    "h2_size = 15\n",
    "output_size = 1\n",
    "# 4 layers\n",
    "model = nn.Sequential(nn.Linear(input_size,h0_size),\n",
    "                    nn.ReLU(),\n",
    "                    \n",
    "                    nn.Linear(h0_size, h1_size),\n",
    "                    nn.ReLU(),\n",
    "                    \n",
    "                    nn.Linear(h1_size, h2_size),\n",
    "                    nn.ReLU(),\n",
    "                    \n",
    "                    nn.Linear(h2_size, output_size))\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 520143.4375\n",
      "1: 493169.96875\n",
      "2: 473774.1875\n",
      "3: 461882.5\n",
      "4: 455548.40625\n",
      "5: 451492.96875\n",
      "6: 446252.3125\n",
      "7: 437401.9375\n",
      "8: 424329.78125\n",
      "9: 408190.875\n",
      "10: 391094.0\n",
      "11: 375203.34375\n",
      "12: 362025.59375\n",
      "13: 352049.40625\n",
      "14: 344846.4375\n",
      "15: 339477.875\n",
      "16: 334840.59375\n",
      "17: 330036.78125\n",
      "18: 324568.34375\n",
      "19: 318342.9375\n",
      "20: 311625.90625\n",
      "21: 304883.59375\n",
      "22: 298598.53125\n",
      "23: 293092.5625\n",
      "24: 288444.3125\n",
      "25: 284481.375\n",
      "26: 280868.46875\n",
      "27: 277224.71875\n",
      "28: 273290.625\n",
      "29: 268999.875\n",
      "30: 264467.78125\n",
      "31: 259924.328125\n",
      "32: 255612.390625\n",
      "33: 251685.875\n",
      "34: 248179.90625\n",
      "35: 245013.765625\n",
      "36: 242042.515625\n",
      "37: 239115.1875\n",
      "38: 236120.140625\n",
      "39: 233014.984375\n",
      "40: 229830.15625\n",
      "41: 226640.828125\n",
      "42: 223530.015625\n",
      "43: 220570.765625\n",
      "44: 217783.1875\n",
      "45: 215147.921875\n",
      "46: 212616.546875\n",
      "47: 210126.515625\n",
      "48: 207628.46875\n",
      "49: 205105.96875\n",
      "50: 202571.625\n",
      "51: 200060.265625\n",
      "52: 197608.375\n",
      "53: 195239.6875\n",
      "54: 192960.890625\n",
      "55: 190758.65625\n",
      "56: 188613.09375\n",
      "57: 186504.015625\n",
      "58: 184422.5625\n",
      "59: 182361.96875\n",
      "60: 180329.625\n",
      "61: 178336.46875\n",
      "62: 176391.15625\n",
      "63: 174495.03125\n",
      "64: 172644.25\n",
      "65: 170831.1875\n",
      "66: 169046.875\n",
      "67: 167286.171875\n",
      "68: 165549.65625\n",
      "69: 163837.34375\n",
      "70: 162152.46875\n",
      "71: 160499.21875\n",
      "72: 158882.953125\n",
      "73: 157302.125\n",
      "74: 155756.0625\n",
      "75: 154236.6875\n",
      "76: 152741.734375\n",
      "77: 151267.6875\n",
      "78: 149811.828125\n",
      "79: 148374.21875\n",
      "80: 146957.34375\n",
      "81: 145565.46875\n",
      "82: 144197.90625\n",
      "83: 142852.234375\n",
      "84: 141525.046875\n",
      "85: 140213.890625\n",
      "86: 138918.34375\n",
      "87: 137635.703125\n",
      "88: 136367.109375\n",
      "89: 135116.125\n",
      "90: 133883.390625\n",
      "91: 132667.328125\n",
      "92: 131465.828125\n",
      "93: 130277.9375\n",
      "94: 129103.9140625\n",
      "95: 127944.8359375\n",
      "96: 126799.0\n",
      "97: 125665.03125\n",
      "98: 124542.15625\n",
      "99: 123430.5234375\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "losses = []\n",
    "for i in range(100):\n",
    "    y_pred = model(x_train)\n",
    "    loss = loss_func(y_pred, y_train)\n",
    "    losses.append(loss.item())\n",
    "    print(f'{i}: {loss.item()}')\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2001.) tensor([2106.0288], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# performance\n",
    "pred = model(x_test[0])\n",
    "print(y_test[0], pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCELoss On XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "input_units = 2\n",
    "hidden_units = 20\n",
    "output_units = 1\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_units, hidden_units),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_units, output_units),\n",
    "    nn.Sigmoid())\n",
    "loss_function = torch.nn.BCELoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "x = [[0.,0.], [0.,1.], [1.,0.],[1.,1.]]\n",
    "x = torch.tensor(x)\n",
    "y =[[0], [1], [1], [0]]\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "for i in range(100):\n",
    "    # Call to to the model to perform a prediction\n",
    "    y_pred = model(x)\n",
    "    # Calculation of loss function based on y_pred and y\n",
    "    y = y.type_as(y_pred)\n",
    "    loss = loss_function(y_pred, y)\n",
    "    # Zero the gradients so that previous ones don't accumulate\n",
    "    optimizer.zero_grad()\n",
    "    # Calculate the gradients of the loss function\n",
    "    loss.backward()\n",
    "    # Call to the optimizer to perform an update of the parameters\n",
    "    optimizer.step()\n",
    "    # print('loss ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true y\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "\n",
      "y_pred\n",
      "[[0.13556355]\n",
      " [0.8504423 ]\n",
      " [0.84838325]\n",
      " [0.15160818]]\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "print('true y')\n",
    "print(y.detach().numpy())\n",
    "print()\n",
    "print('y_pred')\n",
    "print(y_pred.detach().numpy())   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
