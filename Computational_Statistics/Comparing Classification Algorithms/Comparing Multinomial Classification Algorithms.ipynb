{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Classification\n",
    "## Softmax Regression From Scratch\n",
    "This blog uses softmax regression on the iris data set to predict what type of flower each sample is based on 4 different features. I have added to this blog's code a from scratch train_test_spilt function, training and prediction functions, as well as the k-fold cross validation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2020-09-18 19:52:03--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4551 (4.4K) [application/x-httpd-php]\n",
      "Saving to: 'dataset.csv'\n",
      "\n",
      "     0K ....                                                  100% 8.20M=0.001s\n",
      "\n",
      "2020-09-18 19:52:04 (8.20 MB/s) - 'dataset.csv' saved [4551/4551]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1,3.5,1.4,0.2,Iris-setosa\n",
      "4.9,3.0,1.4,0.2,Iris-setosa\n",
      "4.7,3.2,1.3,0.2,Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "!wget -O dataset.csv https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
    "!head -3 dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length in cm</th>\n",
       "      <th>sepal width in cm</th>\n",
       "      <th>petal length in cm</th>\n",
       "      <th>petal width in cm</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length in cm  sepal width in cm  petal length in cm  \\\n",
       "0                 5.1                3.5                 1.4   \n",
       "1                 4.9                3.0                 1.4   \n",
       "2                 4.7                3.2                 1.3   \n",
       "3                 4.6                3.1                 1.5   \n",
       "4                 5.0                3.6                 1.4   \n",
       "\n",
       "   petal width in cm        class  \n",
       "0                0.2  Iris-setosa  \n",
       "1                0.2  Iris-setosa  \n",
       "2                0.2  Iris-setosa  \n",
       "3                0.2  Iris-setosa  \n",
       "4                0.2  Iris-setosa  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv', names=[\n",
    "  \"sepal length in cm\",\n",
    "  \"sepal width in cm\",\n",
    "  \"petal length in cm\",\n",
    "  \"petal width in cm\",\n",
    "  \"class\"\n",
    "])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features\n",
    "X = df[[\"sepal length in cm\",\n",
    "  \"sepal width in cm\",\n",
    "  \"petal length in cm\",\n",
    "  \"petal width in cm\"\n",
    "]].values.astype(np.float32)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class --> Numeric values\n",
    "y = pd.factorize(df['class'])[0]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias factor\n",
    "X = np.hstack((np.ones((len(X), 1)), X))\n",
    "\n",
    "m, n = X.shape\n",
    "K = 3 # Classes\n",
    "K, m, n\n",
    "\n",
    "# Standardization\n",
    "X[:, 1:] = (X[:, 1:] - np.mean(X[:, 1:], axis=0)) / np.std(X[:, 1:], axis=0)\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "def h(X, theta):\n",
    "    return softmax(X @ theta)\n",
    "\n",
    "def J(preds, y):\n",
    "    m = preds.shape[0]\n",
    "    return np.sum(- np.log(preds[np.arange(m), y]))\n",
    "\n",
    "def T(y, K):\n",
    "    \"\"\" one hot encoding \"\"\"\n",
    "    one_hot = np.zeros((len(y), K))\n",
    "    one_hot[np.arange(len(y)), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "def compute_gradient(theta, X, y):\n",
    "    preds = h(X, theta)\n",
    "    gradient = 1/m * X.T @ (preds - T(y, K))\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data Up\n",
    "To test properly on new data, I implemented a train_test_split function from scratch (versus using an sklearn function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (x,y) shape: (120, 5) (120,) \n",
      "Test (x,y) shape: (29, 5) (29,)\n"
     ]
    }
   ],
   "source": [
    "def my_train_test_split(X, y, train_perc):\n",
    "    train_end_ind = int(X.shape[0] * train_perc)  \n",
    "    train_x = X[0:train_end_ind]\n",
    "    train_y = y[0:train_end_ind]\n",
    "    test_x = X[train_end_ind:-1]\n",
    "    test_y = y[train_end_ind:-1]\n",
    "\n",
    "    return (train_x, train_y, test_x, test_y)\n",
    "\n",
    "\n",
    "train_x, train_y, test_x, test_y = my_train_test_split(X, y, 0.8)\n",
    "print(\"Train (x,y) shape:\", train_x.shape, train_y.shape, \"\\nTest (x,y) shape:\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (loss, acc): (756.2966836904976, 0.7916666666666666)\n",
      "Testing (loss, acc): (153.744603026516, 0.034482758620689655)\n"
     ]
    }
   ],
   "source": [
    "def train_from_scratch(X, y, iters, alpha):\n",
    "    theta = np.random.random((n, K))\n",
    "    hist = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for i in range(iters):\n",
    "        \n",
    "        gradient = compute_gradient(theta, X, y)\n",
    "        theta -= alpha * gradient \n",
    "\n",
    "        # loss\n",
    "        preds = h(X, theta) \n",
    "        loss = J(preds, y)\n",
    "        hist['loss'].append(loss) \n",
    "\n",
    "        # acc\n",
    "        c = 0 \n",
    "        for j in range(len(y)): \n",
    "            if np.argmax(h(X[j], theta)) == y[j]: \n",
    "                c += 1 \n",
    "        acc = c / len(y) \n",
    "        hist['acc'].append(acc) \n",
    "\n",
    "    return (loss, acc, theta, hist)\n",
    "\n",
    "def test_from_scratch(X, y, theta):\n",
    "    # loss\n",
    "    preds = h(X, theta) \n",
    "    loss = J(preds, y)\n",
    "\n",
    "    # acc\n",
    "    c = 0 \n",
    "    for j in range(len(y)): \n",
    "        if np.argmax(h(X[j], theta)) == y[j]: \n",
    "            c += 1 \n",
    "    acc = c / len(y) \n",
    "\n",
    "    return (loss,acc)\n",
    "\n",
    "iters = 1500 \n",
    "alpha = 1e-3 \n",
    "\n",
    "train_loss_from_scratch, train_acc_from_scratch, theta, hist = train_from_scratch(train_x, train_y, iters, alpha)\n",
    "test_loss_from_scratch, test_acc_from_scratch = test_from_scratch(test_x, test_y, theta)\n",
    "\n",
    "print(f'Training (loss, acc): {(train_loss_from_scratch,train_acc_from_scratch)}')\n",
    "print(f'Testing (loss, acc): {(test_loss_from_scratch, test_acc_from_scratch)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Mean Accuracy: 0.953 (0.043)\n"
     ]
    }
   ],
   "source": [
    "# No fancy features we're used in the sklearn version so that we are comparing apples to apples\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Init model\n",
    "LR = LogisticRegression(random_state=0)\n",
    "# 10-fold cv using sklearn\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
    "scores = cross_val_score(LR, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "kfold_sklearn_acc = mean(scores)\n",
    "# Performance\n",
    "print('LogisticRegression Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation Comparisons\n",
    "We are unable to compare the from scratch and sklearn multinomial classification algorithms using the packaged pair t-test with 5x2 cross validation since it takes two sklearn models as parameters for comparison. So, we have settled to just comparing the 10-fold cross validation of each.\n",
    "\n",
    "For the sake of learning, I have included how to use the pair t-test with 5x2 cross validation by comparing LR and LDA to show how to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_Fold Cross Validation\n",
    "def cv_split(X, y, start_ind_test, end_ind_test, end_cap):\n",
    "    # k-1 train segments\n",
    "    train_x = np.append(X[0:start_ind_test], X[end_ind_test:end_cap], axis=0)\n",
    "    train_y = np.append(y[0:start_ind_test], y[end_ind_test:end_cap], axis=0)\n",
    "    # 1 test segment\n",
    "    test_x = X[start_ind_test:end_ind_test]\n",
    "    test_y = y[start_ind_test:end_ind_test]\n",
    "\n",
    "    \n",
    "    return (train_x, train_y, test_x, test_y)\n",
    "\n",
    "def kfold_cv_from_scratch(train_func, test_func, X, y, iters, alpha, folds):\n",
    "    hist = {'loss': [], 'acc': []} # Performance history\n",
    "    fold_size = (X.shape[0]) // folds\n",
    "    for i in range(folds):\n",
    "        start_ind_test = i * fold_size\n",
    "        end_ind_test = min(len(X) - 1, start_ind_test + fold_size)\n",
    "        end_cap = fold_size * folds\n",
    "\n",
    "        train_x, train_y, test_x, test_y = cv_split(X, y, start_ind_test, end_ind_test, end_cap)\n",
    "        \n",
    "        _, _, theta, _  = train_func(train_x, train_y, iters, alpha)\n",
    "        loss, acc = test_func(test_x, test_y, theta)\n",
    "        \n",
    "        hist['loss'].append(loss)\n",
    "        hist['acc'].append(acc)\n",
    "    \n",
    "    avg_loss, avg_acc = mean(hist['loss']), mean(hist['acc'])\n",
    "    \n",
    "    return (avg_loss, avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Classification: Softmax From Scratch vs Multinomial Logistic sklearn\n",
    "We can see that the sklearn implementation of Multinomial LR performs much better than the from scratch softmax algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folds = 10\n",
    "_, kfold_from_scratch_acc = kfold_cv_from_scratch(train_from_scratch, test_from_scratch, X, y, iters, alpha, folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy (From Scratch, SKLearn): (0.7185714285714285, 0.9533333333333334), Difference: -0.23476190476190484\n"
     ]
    }
   ],
   "source": [
    "dif = kfold_from_scratch_acc - kfold_sklearn_acc\n",
    "print(f'Mean Accuracy (From Scratch, SKLearn): {kfold_from_scratch_acc, kfold_sklearn_acc}, Difference: {dif}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired T-Test 5x2 Cross Validation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.122, t-Statistic: -1.861\n",
      "Algorithms probably have the same performance\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "t,p = paired_ttest_5x2cv(estimator1=LR, estimator2=LDA, X=X, y=y, scoring='accuracy', random_seed=1)\n",
    "# summarize\n",
    "print('P-value: %.3f, t-Statistic: %.3f' % (p, t))\n",
    "# interpret the result\n",
    "if p <= 0.05:\n",
    "    print('Difference between mean performance is probably real')\n",
    "else:\n",
    "    print('Algorithms probably have the same performance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
